{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7184fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the file and specify which column is the date\n",
    "convert_dates = pd.read_excel(\"publn_dataset.xlsx\")\n",
    "\n",
    "# Output with dates converted to YYYY-MM-DD\n",
    "convert_dates[\"Text4\"] = pd.to_datetime(convert_dates[\"Text4\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "convert_dates[\"Text4\"] = \"CF:D:\" + convert_dates[\"Text4\"]\n",
    "convert_dates.to_excel(\"publn_dataset1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb60342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# reading csv file from url\n",
    "data = pd.read_excel(\"publn_dataset1.xlsx\")\n",
    " \n",
    "# dropping null value columns to avoid errors\n",
    "#data.dropna(inplace = True)\n",
    " \n",
    "# new data frame with split value columns\n",
    "new = data[\"Text1\"].str.split(\" \", n = 1, expand = True)\n",
    " \n",
    "# making separate first name column from new data frame\n",
    "data[\"Text1\"]= new[0]\n",
    " \n",
    "# making separate last name column from new data frame\n",
    " \n",
    "# Dropping old Name columns\n",
    "#data.drop(columns =[\"Name\"], inplace = True)\n",
    " \n",
    "# df display\n",
    "data.to_excel(\"publn_dataset2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703b98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# reading csv file from url\n",
    "data = pd.read_excel(\"publn_dataset2.xlsx\")\n",
    " \n",
    "# dropping null value columns to avoid errors\n",
    "#data.dropna(inplace = True)\n",
    " \n",
    "# new data frame with split value columns\n",
    "new = data[\"Text2\"].str.split(\" \", n = 1, expand = True)\n",
    " \n",
    "# making separate first name column from new data frame\n",
    "data[\"Text2\"]= new[0]\n",
    " \n",
    "# making separate last name column from new data frame\n",
    " \n",
    "# Dropping old Name columns\n",
    "#data.drop(columns =[\"Name\"], inplace = True)\n",
    " \n",
    "# df display\n",
    "data.to_excel(\"publn_dataset3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# reading csv file from url\n",
    "data = pd.read_excel(\"publn_dataset3.xlsx\")\n",
    " \n",
    "# dropping null value columns to avoid errors\n",
    "#data.dropna(inplace = True)\n",
    " \n",
    "# new data frame with split value columns\n",
    "new = data[\"Text3\"].str.split(\" \", n = 1, expand = True)\n",
    " \n",
    "# making separate first name column from new data frame\n",
    "data[\"Text3\"]= new[0]\n",
    " \n",
    "# making separate last name column from new data frame\n",
    " \n",
    "# Dropping old Name columns\n",
    "#data.drop(columns =[\"Name\"], inplace = True)\n",
    " \n",
    "# df display\n",
    "data.to_excel(\"publn_dataset4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d28c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "fileName = 'publn_dataset5.xlsx'\n",
    "df = pd.read_excel(fileName)\n",
    "#df.dropna(inplace = True)\n",
    "df = df[df[\"Text1\"].notna()]\n",
    "df[\"Text1\"] = df[\"Text1\"].replace({\"k\":\"*1e3\"}, regex=True).map(pd.eval).astype(int)\n",
    "df.to_excel(\"removedk.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26230718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "fileName = 'removedk.xlsx'\n",
    "df = pd.read_excel(fileName)\n",
    "#df.dropna(inplace = True)\n",
    "df = df[df[\"Text2\"].notna()]\n",
    "df[\"Text2\"] = df[\"Text2\"].replace({\"k\":\"*1e3\"}, regex=True)\n",
    "df.to_excel(\"removedk1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f109efcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_25276/361518580.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_25276/361518580.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ++++++++++++++++++++++#use this++++++++++++++++++++++++++++++\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "++++++++++++++++++++++#use this++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a95a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "fileName = 'publn_dataset2.xlsx'\n",
    "df1 = pd.read_excel(fileName)\n",
    "#df.dropna(inplace = True)\n",
    "df = df[df[\"Text3\"].notna()]\n",
    "df[\"Text3\"] = df[\"Text3\"].replace({\"k\":\"*1e3\"}, regex=True)\n",
    "df.to_excel(\"removedk3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "fileName = 'publn_dataset2.xlsx'\n",
    "df1 = pd.read_excel(fileName)\n",
    "#df.dropna(inplace = True)\n",
    "df = df[df[\"Text3\"].notna()]\n",
    "df[\"Text3\"] = df[\"Text3\"].replace({\"Metrics\\ndetails\":\"0\"}, regex=True)\n",
    "df.to_excel(\"removedk5.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "fileName = 'removedk3.xlsx'\n",
    "df = pd.read_excel(fileName)\n",
    "df[\"Text3\"] = df[\"Text3\"].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "fileName = 'removedk5.xlsx'\n",
    "df = pd.read_excel(fileName)\n",
    "#df[\"Text3\"] = df[\"Text3\"].astype(str).astype(int)\n",
    "df_std = df.copy()\n",
    "df_std[\"Text1\"] = (df_std[\"Text1\"] - df_std[\"Text1\"].mean()) / df_std[\"Text1\"].std()\n",
    "df_std[\"Text2\"] = (df_std[\"Text2\"] - df_std[\"Text2\"].mean()) / df_std[\"Text2\"].std()\n",
    "df_std[\"Text3\"] = (df_std[\"Text3\"] - df_std[\"Text3\"].mean()) / df_std[\"Text3\"].std()\n",
    "df_std.to_excel(\"removedk6.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22372060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = ['the', 'a', 'an', 'and', 'or', 'in', 'of', 'to', 'for', 'with', 'on', 'at', 'by', 'from']\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('file.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Apply the preprocessing function to the text column\n",
    "df['preprocessed_text'] = df['Text3'].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed data to a new Excel file\n",
    "df.to_excel('preprocessed_file.xlsx', sheet_name='Sheet1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# load research paper titles from Excel sheet\n",
    "df = pd.read_excel('name.xlsx')\n",
    "\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "# extract research paper titles\n",
    "titles = df['Text'].tolist()\n",
    "\n",
    "# initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# vectorize research paper titles\n",
    "X = vectorizer.fit_transform(titles)\n",
    "\n",
    "# initialize KMeans clustering algorithm\n",
    "kmeans = KMeans(n_clusters=16, random_state=100)\n",
    "\n",
    "# train KMeans clustering algorithm\n",
    "kmeans.fit(X)\n",
    "\n",
    "# load new research paper titles from Excel sheet\n",
    "new_titles_df = pd.read_excel('file_keywords.xlsx')\n",
    "\n",
    "# create empty list to store predicted domains\n",
    "domains = []\n",
    "\n",
    "# loop through each new research paper title and predict its domain\n",
    "for i, row in new_titles_df.iterrows():\n",
    "    new_title = row['keywords']\n",
    "    if isinstance(new_title, str):\n",
    "        # vectorize new research paper title\n",
    "        new_X = vectorizer.transform([new_title])\n",
    "\n",
    "        # predict domain label of new research paper title\n",
    "        label = kmeans.predict(new_X)[0]\n",
    "\n",
    "        # map domain label to corresponding domain\n",
    "        domain_mapping = {\n",
    "            0: 'Oncology',\n",
    "            1: 'Immunology',\n",
    "            2: 'Cardiology',\n",
    "            3: 'Neurology',\n",
    "            4: 'Pediatrics',\n",
    "            5: 'Epidemiology',\n",
    "            6: 'Pharmacology',\n",
    "            7: 'Genetics',\n",
    "            8: 'Biomedical engineering',\n",
    "            9: 'Anatomy',\n",
    "            10: 'Natural Language Processing',\n",
    "            11: 'Physics',\n",
    "            12: 'Biology',\n",
    "            13: 'Finance',\n",
    "            14: 'Environmental Science',\n",
    "            15: 'Computer Science'\n",
    "        }\n",
    "        domain = domain_mapping[label]\n",
    "    else:\n",
    "        domain = np.nan\n",
    "    domains.append(domain)\n",
    "\n",
    "# add predicted domains to new research paper titles DataFrame\n",
    "new_titles_df['Domain'] = domains\n",
    "\n",
    "# save new research paper titles with predicted domains to Excel sheet\n",
    "new_titles_df.to_excel('new_research_papers_with_domains1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# read Excel sheet into a pandas DataFrame\n",
    "df = pd.read_excel('example_updated3.xlsx')\n",
    "\n",
    "# fill in missing values with an empty string\n",
    "df = df.fillna('')\n",
    "\n",
    "# extract TF-IDF features from the article text\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(df['Text'])\n",
    "tfidf_norm = normalize(tfidf)\n",
    "\n",
    "# apply non-negative matrix factorization (NMF) to cluster the articles into topics\n",
    "nmf = NMF(n_components=10)\n",
    "W = nmf.fit_transform(tfidf_norm)\n",
    "H = nmf.components_\n",
    "\n",
    "# identify the most important topics for each domain\n",
    "domainwise_top_topics = {}\n",
    "for domain in df['Domain'].unique():\n",
    "    # extract articles for the current domain\n",
    "    domain_articles = df[df['Domain'] == domain]\n",
    "\n",
    "    # extract TF-IDF features from the article text for the current domain\n",
    "    domain_tfidf = vectorizer.transform(domain_articles['Text'])\n",
    "    domain_tfidf_norm = normalize(domain_tfidf)\n",
    "\n",
    "    # apply NMF to the TF-IDF features for the current domain\n",
    "    domain_W = nmf.transform(domain_tfidf_norm)\n",
    "\n",
    "    # identify the top topics for the current domain\n",
    "    domain_top_topics = domain_W.sum(axis=0).argsort()[::-1][:5].tolist()\n",
    "    domainwise_top_topics[domain] = domain_top_topics\n",
    "\n",
    "# identify the authors who have written the most articles in the top topics for each domain\n",
    "domainwise_popular_authors = {}\n",
    "for domain, top_topics in domainwise_top_topics.items():\n",
    "    # extract articles that belong to the top topics for the current domain\n",
    "    topic_articles = df[df['Text'].apply(lambda x: any(str(topic) in x for topic in top_topics))]\n",
    "\n",
    "    # group the articles by author and count the articles\n",
    "    author_counts = topic_articles.groupby('Names1')['Text'].count().reset_index()\n",
    "\n",
    "    # sort the authors by the number of articles they have written\n",
    "    popular_authors = author_counts.sort_values('Text', ascending=False)['Names1'].tolist()\n",
    "\n",
    "    # exclude the name 'united' from the list of popular authors\n",
    "    if 'United' in popular_authors:\n",
    "        popular_authors.remove('United')\n",
    "\n",
    "    # select the top 5 authors (excluding 'united') for the current domain\n",
    "    domainwise_popular_authors[domain] = popular_authors[:5]\n",
    "\n",
    "# display the top authors for each domain\n",
    "for domain, authors in domainwise_popular_authors.items():\n",
    "    print(f'Top authors in {domain}: {\", \".join(authors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968534a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"domainnew.csv\")\n",
    "\n",
    "# Calculate summary statistics for citation counts by domain\n",
    "domain_stats = df.groupby(\"Domain\")[\"Citation\"].describe()\n",
    "\n",
    "# Sort domains by median citation count in descending order\n",
    "domain_stats = domain_stats.sort_values(by=\"50%\", ascending=False)\n",
    "\n",
    "# Print top 10 domains by median citation count\n",
    "print(domain_stats.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f10cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"domainnew.csv\")\n",
    "\n",
    "# Conduct ANOVA to test for differences in citation counts between domains\n",
    "results = f_oneway(*[df[df[\"Domain\"]==d][\"Citation\"] for d in df[\"Domain\"].unique()])\n",
    "\n",
    "# Print ANOVA results\n",
    "print(\"F-value: \", results.statistic)\n",
    "print(\"p-value: \", results.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"domainnew.csv\")\n",
    "\n",
    "# Prepare X and y variables for linear regression\n",
    "X = pd.get_dummies(df[\"Domain\"], drop_first=True)\n",
    "y = df[\"Citation\"]\n",
    "\n",
    "# Train linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on new observations\n",
    "new_data = pd.DataFrame({\"Domain\": [\"covid\", \"economics\", \"mechanics\"], \n",
    "                         \"Oncology\" : [0, 0, 0],\n",
    "                         \"Immunology\" : [1, 0 ,0],\n",
    "                         \"Cardiology\": [0, 0, 0],\n",
    "                         \"Neurology\":[0, 0, 0],\n",
    "                         \"Pediatrics\":[0, 0, 0],\n",
    "                         \"Epidemiology\":[0, 0, 0],\n",
    "                         \"Pharmacology\":[1, 0, 0],\n",
    "                         \"Genetics\":[1, 0, 0],\n",
    "                         \"Biomedical engineering\":[1, 0, 0],\n",
    "                         \"Anatomy\":[1, 0, 0],\n",
    "                         \"Natural Language Processing\":[0, 0, 0],\n",
    "                         \"Physics\":[0, 0, 1],\n",
    "                         \"Biology\":[1, 0, 0],\n",
    "                         \"Finance\":[0, 1, 0],\n",
    "                         \"Environmental Science\":[0, 0, 0],\n",
    "                         \"Computer Science\":[0, 0, 0]\n",
    "                        })\n",
    "#new_X = new_data.drop(\"Domain\", axis=1)\n",
    "new_X = new_data[X.columns]\n",
    "predictions = model.predict(new_X)\n",
    "\n",
    "# Print predicted citation counts for new observations\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Predicted citations for {new_data.iloc[i]['Domain']}: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328723cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel sheet into a pandas DataFrame\n",
    "df = pd.read_excel('name.xlsx')\n",
    "\n",
    "# Create an empty list to hold the new data\n",
    "new_data = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Get the name and authors for this row\n",
    "    citation = row['citation']\n",
    "    name = row['article']\n",
    "    author1 = row['author1']\n",
    "    author2 = row['author2']\n",
    "    author3 = row['author3']\n",
    "    \n",
    "    # Add a new dictionary to the list for each author\n",
    "    new_data.append({'Name': name, 'Author': author1, 'citation': citation})\n",
    "    new_data.append({'Name': name, 'Author': author2, 'citation': citation})\n",
    "    new_data.append({'Name': name, 'Author': author3, 'citation': citation})\n",
    "\n",
    "# Create a new DataFrame from the list of dictionaries\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Write the new DataFrame to a new sheet in the Excel file\n",
    "with pd.ExcelWriter('new_file_name1.xlsx') as writer:\n",
    "    new_df.to_excel(writer, sheet_name='Sheet1', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from Excel file\n",
    "#file_path = input(\"Enter the path to the Excel file: \")\n",
    "df = pd.read_excel('name1.xlsx')\n",
    "\n",
    "# Group papers by author\n",
    "grouped_data = df.groupby('Author')\n",
    "\n",
    "# Calculate h-index for each author\n",
    "for author_name, group in grouped_data:\n",
    "    # Sort papers by number of citations in descending order\n",
    "    group_sorted = group.sort_values('citation', ascending=False)\n",
    "\n",
    "    # Calculate h-index\n",
    "    h_index = 0\n",
    "    for i, row in group_sorted.iterrows():\n",
    "        if row['citation'] >= (i + 1):\n",
    "            h_index += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Print the h-index for the current author\n",
    "    print(f\"The h-index of {author_name} is {h_index}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
